import pandas as pd
from numpy import log

def entropy_from_series(s):
    """Compute entropy from a Series

    Args:
        s (Series): Series of counts where p=count/Sum(counts)

    Returns:
        float: Entropy (base e)
    """
    tot = s.sum()
    p = s.div(tot)
    user_entropy = (-p * log(p)).sum()
    return user_entropy


def create_feature_columns(labels, rolling_hrs):
    """_summary_

    Args:
        labels (DataFrame): Labeled data DataFrame to be modified.
        rolling_hrs (int): Rolling average length in hours.

    Returns:
        dict: dictionary of column names (to be passed as kwarg)
    """
    colnames = {
        "hist_user_entropy_colname": f"avg_user_entropy_past_{rolling_hrs}_hrs",
        "hist_user_count_colname": f"avg_user_count_past_{rolling_hrs}_hrs",
        "hist_page_entropy_colname": f"avg_page_entropy_past_{rolling_hrs}_hrs",
        "hist_revision_count_colname": f"avg_revision_count_past_{rolling_hrs}_hrs",
    }

    labels["user_entropy_cur"] = None
    labels[colnames["hist_user_entropy_colname"]] = None

    labels["user_count_cur"] = None
    labels[colnames["hist_user_count_colname"]] = None

    labels["page_entropy_cur"] = None
    labels[colnames["hist_page_entropy_colname"]] = None

    labels[colnames["hist_revision_count_colname"]] = None

    labels["num_pages_cur"] = None
    labels["page_share_cur"] = None
    labels["revision_vol_cur"] = None

    labels["max_burst_score"] = None

    return colnames

def engineer_common_training(df):
    """Do common transformations on columns, e.g. taking logs.

    Args:
        df (DataFrame): The training data.

    Returns:
        DataFrame: The transformed training data (in place).
    """
    df["log_page_share_cur"] = log(df["page_share_cur"])
    return df


def engineer_features_by_event(
    row, idx, counts, page_entropies, revisions, labels, rolling_hrs, **colnames
):
    """Engineer the features for a row (event) at index idx.

    Args:
        row (Series): Row's Series from labels.iterrows()
        idx (int): Index from labels.iterrows()
        counts (DataFrame): Binned/counted data.
        page_entropies (DataFrame): Pre-computed page entropy statistics.
        revisions (DataFrame): Dictionary of revision data.
        labels (DataFrame): labeled data
        rolling_hrs (int): rolling average span in hours
        colnames (dict): names of the historical average columns,
                         generated by create_feature_columns
    """
    ts2 = row["event_timestamp"]
    ts1 = ts2 - pd.Timedelta(hours=rolling_hrs - 1)

    # Revisions for this page in this time period
    filtered = revisions[
        (
            revisions.event_timestamp.between(
                ts1, ts2 + pd.Timedelta(hours=1), inclusive="right"
            )
        )
        & (revisions.page_id == row.page_id)
    ]

    # Revisions binned by user and hourly timestamp
    binned_filtered = (
        filtered.groupby(
            [pd.Grouper(key="event_timestamp", freq="h"), "event_user_text"]
        )
        .agg(num_edits_by_user=("event_timestamp", "size"))
        .reset_index()
    )

    # Count the number of unique users in each timestamp and compute historical average
    unique_users = binned_filtered.groupby("event_timestamp").agg(
        num_unique_users=("event_timestamp", "size")
    )["num_unique_users"]
    labels.loc[idx, colnames["hist_user_count_colname"]] = unique_users.iloc[
        :-1
    ].sum() / (rolling_hrs - 1)
    labels.loc[idx, "user_count_cur"] = unique_users.iloc[-1]

    # Compute & set user entropy
    binned_filtered["user_entropy_per_hour"] = binned_filtered.groupby(
        "event_timestamp"
    )["num_edits_by_user"].transform(entropy_from_series)
    labels.loc[idx, "user_entropy_cur"] = binned_filtered.tail(1)[
        "user_entropy_per_hour"
    ].values

    _ = (
        binned_filtered.groupby("event_timestamp")
        .agg(test=("user_entropy_per_hour", "first"))
        .reset_index()["test"]
        .iloc[:-1]
        .values
    )
    if _.size == 0:
        labels.loc[idx, colnames["hist_user_entropy_colname"]] = 0
    else:
        labels.loc[idx, colnames["hist_user_entropy_colname"]] = _.mean()

    # Retrieve the corresponding current page entropy from page_entropies
    labels.loc[idx, "page_entropy_cur"] = page_entropies.loc[
        row["event_timestamp"]
    ].values
    labels.loc[idx, colnames["hist_page_entropy_colname"]] = page_entropies.loc[
        ts1 : ts2 - pd.Timedelta(hours=1)
    ].values.mean()

    # Retrieve the total number of pages being edited from counts
    labels.loc[idx, "num_pages_cur"] = (
        counts.groupby(["event_timestamp"])
        .agg(num_page_cur=("page_title", "size"))["num_page_cur"]
        .loc[ts2]
    )

    # Retrieve the total revision volume in this timestamp from counts
    # Compute the historical revision volume
    # Compute this page's share of the revision volume
    labels.loc[idx, "revision_vol_cur"] = (
        counts.groupby(["event_timestamp"])
        .agg(revision_vol_cur=("revision_count", "sum"))["revision_vol_cur"]
        .loc[ts2]
    )

    labels.loc[idx, colnames["hist_revision_count_colname"]] = counts[
        (counts.page_title == row["page_title"])
        & (counts.event_timestamp.between(ts1, ts2 - pd.Timedelta(hours=1)))
    ].num_unique_users.sum() / (rolling_hrs - 1)

    # TODO: fix the typing of the columns so Pylance doesn't throw an error here
    labels.loc[idx, "page_share_cur"] = labels.loc[idx, "revision_count"].astype(
        "float"
    ) / labels.loc[idx, "revision_vol_cur"].astype("float")

    # Compute the max burstiness score for 5 minute periods in the past hour
    filtered_past1hr = filtered[
        filtered.event_timestamp.between(
            ts2 - pd.Timedelta(hours=1), ts2 + pd.Timedelta(hours=1)
        )
    ]
    # TODO: Make this mutable w/ arguments passed
    binned_past1hr = filtered_past1hr.groupby(
        pd.Grouper(key="event_timestamp", freq="5Min")
    ).agg(revision_count=("event_timestamp", "size"))
    labels.loc[idx, "max_burst_score"] = (
        binned_past1hr["revision_count"].div(labels.loc[idx, "revision_count"]) - 1 / 12
    ).max()