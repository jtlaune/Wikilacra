"""
Transform and feature engineer labeled data.
"""

import sys
import pandas as pd
from numpy import log
from wikilacra import MEDIAWIKI_HISTOR_DUMP_COL_NAMES
from wikilacra.stream import bin_and_count, read_data_chunked
from label_training_data import load_and_clean


def load_labeled_data(filepath, start_dt, rolling_hrs):
    """Load the labels, preprocess, and return.

    Args:
        filepath (str): Path of input labeled data file.
        start_dt (str): Datetime string of start time, e.g. 2025-08-01 00:00:00
        offset_hrs (int): Number of hours for rolling averages
    """
    labels = pd.read_csv(filepath, index_col=0)
    labels.rename(
        columns={
            "EVENT, EDIT_WAR, VANDALISM, NONE, MOVED_OR_DELETED": "Category",
        },
        inplace=True,
    )
    labels["event_timestamp"] = pd.to_datetime(labels["event_timestamp"])

    # Since we're going to be using rolling_hrs-hour moving averages
    labels = labels[
        labels.event_timestamp
        >= pd.to_datetime(start_dt) + pd.Timedelta(hours=rolling_hrs)
    ]

    return labels


def entropy_from_series(s):
    """Compute entropy from a Series

    Args:
        s (Series): Series of counts where p=count/Sum(counts)

    Returns:
        float: Entropy (base e)
    """
    tot = s.sum()
    p = s.div(tot)
    user_entropy = (-p * log(p)).sum()
    return user_entropy


def create_feature_columns(labels, rolling_hrs):
    """_summary_

    Args:
        labels (DataFrame): Labeled data DataFrame to be modified.
        rolling_hrs (int): Rolling average length in hours.

    Returns:
        dict: dictionary of column names (to be passed as kwarg)
    """
    colnames = {
        "hist_user_entropy_colname": f"avg_user_entropy_past_{rolling_hrs}_hrs",
        "hist_user_count_colname": f"avg_user_count_past_{rolling_hrs}_hrs",
        "hist_page_entropy_colname": f"avg_page_entropy_past_{rolling_hrs}_hrs",
        "hist_revision_count_colname": f"avg_revision_count_past_{rolling_hrs}_hrs",
    }

    labels["user_entropy_cur"] = None
    labels[colnames["hist_user_entropy_colname"]] = None

    labels["user_count_cur"] = None
    labels[colnames["hist_user_count_colname"]] = None

    labels["page_entropy_cur"] = None
    labels[colnames["hist_page_entropy_colname"]] = None

    labels[colnames["hist_revision_count_colname"]] = None

    labels["num_pages_cur"] = None
    labels["page_share_cur"] = None
    labels["revision_vol_cur"] = None

    labels["max_burst_score"] = None

    return colnames


def engineer_features_by_event(
    row, idx, counts, page_entropies, revisions, labels, rolling_hrs, **colnames
):
    """Engineer the features for a row (event) at index idx.

    Args:
        row (Series): Row's Series from labels.iterrows()
        idx (int): Index from labels.iterrows()
        counts (DataFrame): Binned/counted data.
        page_entropies (DataFrame): Pre-computed page entropy statistics.
        revisions (DataFrame): Dictionary of revision data.
        labels (DataFrame): labeled data
        rolling_hrs (int): rolling average span in hours
        colnames (dict): names of the historical average columns,
                         generated by create_feature_columns
    """
    ts2 = row["event_timestamp"]
    ts1 = ts2 - pd.Timedelta(hours=rolling_hrs - 1)

    # Revisions for this page in this time period
    filtered = revisions[
        (
            revisions.event_timestamp.between(
                ts1, ts2 + pd.Timedelta(hours=1), inclusive="right"
            )
        )
        & (revisions.page_id == row.page_id)
    ]

    # Revisions binned by user and hourly timestamp
    binned_filtered = (
        filtered.groupby(
            [pd.Grouper(key="event_timestamp", freq="h"), "event_user_text"]
        )
        .agg(num_edits_by_user=("event_timestamp", "size"))
        .reset_index()
    )

    # Count the number of unique users in each timestamp and compute historical average
    unique_users = binned_filtered.groupby("event_timestamp").agg(
        num_unique_users=("event_timestamp", "size")
    )["num_unique_users"]
    labels.loc[idx, colnames["hist_user_count_colname"]] = unique_users.iloc[
        :-1
    ].sum() / (rolling_hrs - 1)
    labels.loc[idx, "user_count_cur"] = unique_users.iloc[-1]

    # Compute & set user entropy
    binned_filtered["user_entropy_per_hour"] = binned_filtered.groupby(
        "event_timestamp"
    )["num_edits_by_user"].transform(entropy_from_series)
    labels.loc[idx, "user_entropy_cur"] = binned_filtered.tail(1)[
        "user_entropy_per_hour"
    ].values

    _ = (
        binned_filtered.groupby("event_timestamp")
        .agg(test=("user_entropy_per_hour", "first"))
        .reset_index()["test"]
        .iloc[:-1]
        .values
    )
    if _.size == 0:
        labels.loc[idx, colnames["hist_user_entropy_colname"]] = 0
    else:
        labels.loc[idx, colnames["hist_user_entropy_colname"]] = _.mean()

    # Retrieve the corresponding current page entropy from page_entropies
    labels.loc[idx, "page_entropy_cur"] = page_entropies.loc[
        row["event_timestamp"]
    ].values
    labels.loc[idx, colnames["hist_page_entropy_colname"]] = page_entropies.loc[
        ts1 : ts2 - pd.Timedelta(hours=1)
    ].values.mean()

    # Retrieve the total number of pages being edited from counts
    labels.loc[idx, "num_pages_cur"] = (
        counts.groupby(["event_timestamp"])
        .agg(num_page_cur=("page_title", "size"))["num_page_cur"]
        .loc[ts2]
    )

    # Retrieve the total revision volume in this timestamp from counts
    # Compute the historical revision volume
    # Compute this page's share of the revision volume
    labels.loc[idx, "revision_vol_cur"] = (
        counts.groupby(["event_timestamp"])
        .agg(revision_vol_cur=("revision_count", "sum"))["revision_vol_cur"]
        .loc[ts2]
    )

    labels.loc[idx, colnames["hist_revision_count_colname"]] = counts[
        (counts.page_title == row["page_title"])
        & (counts.event_timestamp.between(ts1, ts2 - pd.Timedelta(hours=1)))
    ].num_unique_users.sum() / (rolling_hrs - 1)

    # TODO: fix the typing of the columns so Pylance doesn't throw an error here
    labels.loc[idx, "page_share_cur"] = labels.loc[idx, "revision_count"].astype(
        "float"
    ) / labels.loc[idx, "revision_vol_cur"].astype("float")

    # Compute the max burstiness score for 5 minute periods in the past hour
    filtered_past1hr = filtered[
        filtered.event_timestamp.between(
            ts2 - pd.Timedelta(hours=1), ts2 + pd.Timedelta(hours=1)
        )
    ]
    # TODO: Make this mutable w/ arguments passed
    binned_past1hr = filtered_past1hr.groupby(
        pd.Grouper(key="event_timestamp", freq="5Min")
    ).agg(revision_count=("event_timestamp", "size"))
    labels.loc[idx, "max_burst_score"] = (
        binned_past1hr["revision_count"].div(labels.loc[idx, "revision_count"]) - 1 / 12
    ).max()


if __name__ == "__main__":
    rolling_avg_hrs = int(sys.argv[1])
    dump_input_file = sys.argv[2]
    labels_input_file = sys.argv[3]
    data_start_dt = sys.argv[4]
    bin_freq = sys.argv[5]
    output_path = sys.argv[6]

    non_event = {"EDIT_WAR", "VANDALISM", "NONE", "MOVED_OR_DELETED"}

    labels = load_labeled_data(labels_input_file, data_start_dt, rolling_avg_hrs)
    columns_to_keep = [
        "event_timestamp",
        "page_title",
        "event_user_id",
        "event_user_text",
        "page_id",
        "page_is_deleted",
        "event_comment",
    ]
    columns_to_read = [
        "event_entity",
        "page_namespace",
        *columns_to_keep,
    ]
    revisions = load_and_clean(
        dump_input_file,
        columns_to_keep,
        columns_to_read,
    )
    counts = bin_and_count(revisions, bin_freq)

    colnames = create_feature_columns(labels, rolling_avg_hrs)

    page_entropies = counts.groupby("event_timestamp").agg(
        test=("revision_count", entropy_from_series)
    )
    for idx, row in labels.iterrows():
        engineer_features_by_event(
            row,
            idx,
            counts,
            page_entropies,
            revisions,
            labels,
            rolling_avg_hrs,
            **colnames,
        )

    labels.to_csv(output_path)
